{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coded by ZHANG Junjie (University of Science and Technology of China) in 09/2018.\n",
    "\n",
    "This program is free: you can redistribute it and/or modify it under the terms of \n",
    "the Apache License Version 2.0, January 2004 (http://www.apache.org/licenses/).\n",
    "\n",
    "The program requires python tensorflow and numpy to be pre-installed in your \n",
    "GPU-supported computer. \n",
    "\n",
    "'''\n",
    "ZMCIntegral_VERSION='1.0'\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import multiprocessing\n",
    "\n",
    "# detect if GPU is available on the computer\n",
    "def is_gpu_available(cuda_only=True):\n",
    "    \n",
    "    from tensorflow.python.client import device_lib as _device_lib\n",
    "    \n",
    "    if cuda_only:\n",
    "        gpu_available=[int(x.name[-1]) for x in _device_lib.list_local_devices() if (x.device_type == 'GPU')]\n",
    "        np.save(os.getcwd()+'/multi_temp/gpu_available', gpu_available)\n",
    "    else:\n",
    "        gpu_available=[int(x.name[-1]) for x in _device_lib.list_local_devices() if (x.device_type == 'GPU' or x.device_type == 'SYCL')]\n",
    "        np.save(os.getcwd()+'/multi_temp/gpu_available', gpu_available)\n",
    "        \n",
    "def convert_1d_to_nd(one_d, dim, system_digit):\n",
    "    '''convert one_d number to n_d of arbitrary systems\n",
    "    system_digit: int, 2 means binary, 10 means the usual system'''\n",
    "    import numpy as np\n",
    "    import math\n",
    "    \n",
    "    temp_point=np.zeros(dim)\n",
    "    for i_dim in range(dim):\n",
    "        temp_i_one_d=one_d\n",
    "        for temp_dim in range(dim):\n",
    "            temp_i_one_d=temp_i_one_d-temp_point[temp_dim]*(system_digit**(dim-temp_dim-1))\n",
    "        temp_point[i_dim]=math.floor(temp_i_one_d/(system_digit**(dim-i_dim-1)))\n",
    "    return temp_point\n",
    "\n",
    "def MCkernel(n_batches,domain,n_chunk_x,dim,my_func,n_grid_x_one_chunk,n_chunk,batch_size,i_batch):\n",
    "    '''multiprocessing MC integration on different GPU\n",
    "    n_batches: number of available GPUs\n",
    "    domain: domain of the integral [[a,b],...]\n",
    "    n_chunk_x: number of chunks along one dimension\n",
    "    dim: dimensional of the integral\n",
    "    my_func: user defined function\n",
    "    n_grid_x_one_chunk: number of grid along one dimension in each chunk\n",
    "    n_chunk: total number of chunks\n",
    "    '''\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.eager.context import context, EAGER_MODE, GRAPH_MODE\n",
    "    import os,sys\n",
    "    \n",
    "    def switch_to(mode):\n",
    "        ctx = context()._eager_context\n",
    "        ctx.mode = mode\n",
    "        ctx.is_eager = mode == EAGER_MODE\n",
    "    switch_to(EAGER_MODE)\n",
    "    assert tf.executing_eagerly()\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(i_batch)\n",
    "    \n",
    "    MCresult = 0.\n",
    "    for i_chunk in range(batch_size):\n",
    "        chunk_id = i_chunk + i_batch*batch_size\n",
    "        if chunk_id<n_chunk:\n",
    "            chunk_id_d_dim = convert_1d_to_nd(chunk_id,dim,n_chunk_x)\n",
    "             \n",
    "            domain_range=np.array([(domain[idim][1]-domain[idim][0])/n_chunk_x for idim in range(dim)],dtype=np.float32)\n",
    "            domain_left=np.array([domain[idim][0]+chunk_id_d_dim[idim]*domain_range[idim] for idim in range(dim)],dtype=np.float32)\n",
    "                \n",
    "            dr_tensor=tf.expand_dims(domain_range,1)\n",
    "            dl_tensor=tf.expand_dims(domain_left,1)\n",
    "                    \n",
    "            # random variables of sampling points\n",
    "            random_domain_values = tf.add(tf.multiply(tf.random_uniform([dim,n_grid_x_one_chunk**dim],dtype=tf.float32),dr_tensor),dl_tensor)\n",
    "            random_domain_values = list(map(tf.squeeze,tf.split(random_domain_values,dim,0),[0 for i in range(dim)]))\n",
    "            \n",
    "            # user defined function\n",
    "            user_func = my_func(random_domain_values)\n",
    "            \n",
    "            # suppress singularities into 0.0\n",
    "            user_func = tf.where(tf.is_nan(user_func), tf.zeros_like(user_func,dtype=tf.float32), user_func)\n",
    "            user_func = tf.where(tf.is_inf(user_func), tf.zeros_like(user_func,dtype=tf.float32), user_func)\n",
    "            \n",
    "            # monte carlo result in this small chunk\n",
    "            MCresult += tf.scalar_mul(np.prod(domain_range,dtype=np.float32),tf.reduce_mean(user_func,axis=-1)).numpy()\n",
    "             \n",
    "    return MCresult  \n",
    "\n",
    "class MCintegral():\n",
    "    '''\n",
    "    my_func: user defined multidimansional function, type:function\n",
    "    domain: integration domain, type:list/numpy_array\n",
    "    available_GPU: list of available gpus, type: list, Default:All GPUs detected\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,my_func=None,domain=None,available_GPU=None):\n",
    "        \n",
    "        folder = os.getcwd()+'/multi_temp/'\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        \n",
    "        # clean temp file\n",
    "        for the_file in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, the_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        \n",
    "        # check gpu condition\n",
    "        p= multiprocessing.Process(target=is_gpu_available)\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        p.join()\n",
    "        \n",
    "        available_GPU=np.load(os.getcwd()+'/multi_temp/gpu_available.npy')\n",
    "        if len(available_GPU) == 0:\n",
    "            raise AssertionError(\"Your computer does not support GPU calculation.\")\n",
    "        \n",
    "        self.available_GPU=available_GPU\n",
    "        self.my_func,self.domain,self.dim,self.n_grid,self.chunk_size,self.n_batches = \\\n",
    "            self.initial(my_func,domain)\n",
    "        \n",
    "        self.MCresult = self.evaluate()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        '''Monte Carlo integration.'''\n",
    "        \n",
    "        dim=self.dim\n",
    "        \n",
    "        # n_grid_x: number of boxes along one dimension\n",
    "        n_grid_x=int(np.round(self.n_grid**(1/self.dim)))\n",
    "        \n",
    "        # n_grid_x_one_chunk: number of boxes in one chunk along one dimension\n",
    "        n_grid_x_one_chunk=int(np.round(self.chunk_size**(1/self.dim)))\n",
    "        \n",
    "        # n_chunk: total number of chunks\n",
    "        n_chunk=int(np.round((self.n_grid/self.chunk_size)))\n",
    "        \n",
    "        # n_chunk_x: number of chunks along one dimension\n",
    "        n_chunk_x=int(np.round(((self.n_grid)**(1/self.dim))/(self.chunk_size**(1/self.dim))))\n",
    "        \n",
    "        # batch_size\n",
    "        batch_size=int(np.ceil(n_chunk/self.n_batches))\n",
    "        \n",
    "        p={}\n",
    "        for i_batch in range(self.n_batches):\n",
    "            def multi_processing():\n",
    "                result=MCkernel(self.n_batches,self.domain,n_chunk_x,dim,self.my_func,n_grid_x_one_chunk,n_chunk,batch_size,i_batch)\n",
    "                np.save(os.getcwd()+'/multi_temp/result'+str(i_batch), result)\n",
    "                \n",
    "            # start multi-processing to allocate     \n",
    "            p[i_batch] = multiprocessing.Process(target=multi_processing)\n",
    "            p[i_batch].daemon = True\n",
    "            p[i_batch].start()\n",
    "            \n",
    "        for i_batch in range(self.n_batches):   \n",
    "            p[i_batch].join()\n",
    "                \n",
    "        MCresult=0.\n",
    "        for i_batch in range(self.n_batches): \n",
    "            MCresult+=np.load(os.getcwd()+'/multi_temp/result'+str(i_batch)+'.npy')\n",
    "            \n",
    "        return MCresult\n",
    "    \n",
    "    def initial(self,my_func,domain):\n",
    "        '''Return proper initial consitions:\n",
    "        dim: number of free variables, int,\n",
    "        chunk_size: number of samplings in each chunk,int,\n",
    "        n_grid: total number of d-dimensional samplings,int\n",
    "        n_batches: seperate data into n_batches parts, int,\n",
    "        '''\n",
    "        \n",
    "        dim=len(domain)\n",
    "        if my_func==None:\n",
    "            raise AssertionError(\"Invalid input function\")\n",
    "        elif domain==None or type(domain)==int:\n",
    "            raise AssertionError(\"Please enter a correct domain\")\n",
    "            \n",
    "        if dim==None:\n",
    "            dim=1\n",
    "        else:\n",
    "            if type(dim)!=int:\n",
    "                raise AssertionError(\"dim must be an int\")\n",
    "        \n",
    "        # chunk_size: one batch contains many chunks\n",
    "        if dim==1:\n",
    "            n_grid=4194304\n",
    "            chunk_size=65536\n",
    "        elif dim==2:\n",
    "            n_grid=(32768)**2\n",
    "            chunk_size=(4096)**2\n",
    "        elif dim==3:\n",
    "            n_grid=(1024)**3\n",
    "            chunk_size=(256)**3\n",
    "        elif dim==4:\n",
    "            n_grid=(192)**4\n",
    "            chunk_size=(64)**4\n",
    "        elif dim==5:\n",
    "            n_grid=(64)**5\n",
    "            chunk_size=(32)**5\n",
    "        elif dim==6:\n",
    "            n_grid=(32)**6\n",
    "            chunk_size=(16)**6\n",
    "        elif dim==7:\n",
    "            n_grid=(20)**7\n",
    "            chunk_size=(10)**7\n",
    "        elif dim==8:\n",
    "            n_grid=(14)**8\n",
    "            chunk_size=(7)**8\n",
    "        elif dim==9:\n",
    "            n_grid=(10)**9\n",
    "            chunk_size=(5)**9\n",
    "        elif dim==10:\n",
    "            n_grid=(8)**dim\n",
    "            chunk_size=(4)**dim\n",
    "        elif dim==11:\n",
    "            n_grid=(6)**dim\n",
    "            chunk_size=(3)**dim\n",
    "        else:\n",
    "            n_grid=(4)**dim\n",
    "            chunk_size=(2)**dim    \n",
    "            \n",
    "        if len(domain)!=dim:\n",
    "            raise AssertionError(\"Domain is inconsistant with dimension\")\n",
    "        for temp in domain:\n",
    "            if len(temp)!=2:\n",
    "                raise AssertionError(\"Domain is incorrect\")\n",
    "            if temp[1]<temp[0]:\n",
    "                raise AssertionError(\"Domain [a,b] should satisfy b>a\")\n",
    "        \n",
    "        # n_batches: number of batches\n",
    "        n_batches=len(self.available_GPU)\n",
    "        \n",
    "        return my_func,domain,dim,n_grid,chunk_size,n_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
