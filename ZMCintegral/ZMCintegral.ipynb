{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coded by ZHANG Junjie (University of Science and Technology of China) in 01/2019.\n",
    "\n",
    "This program is free: you can redistribute it and/or modify it under the terms of \n",
    "the Apache License Version 2.0, January 2004 (http://www.apache.org/licenses/).\n",
    "\n",
    "The program requires python numba to be pre-installed in your \n",
    "GPU-supported computer. \n",
    "\n",
    "'''\n",
    "ZMCIntegral_VERSION = '3.0'\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from numba import cuda\n",
    "import numba as nb\n",
    "import random\n",
    "import os\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float64\n",
    "\n",
    "class MCintegral():\n",
    "    \n",
    "    def __init__(self, my_func = None, domain = None, available_GPU = None, num_trials = 2, depth = 2, sigma_multiplication = 5):\n",
    "\n",
    "        '''\n",
    "        Parameters:\n",
    "            my_func: user defined multidimensional function, type:function\n",
    "            domain: integration domain, type:list/numpy_array, eg [[0,1]] or [[0,1],[0,1]]\n",
    "            available_GPU: list of available gpus, type: list, Default: All GPUs detected, eg [0,1,2,3]\n",
    "            num_trial: number of trials, type:int, Default:2\n",
    "            depth: search depth, type:int, Default:2\n",
    "            sigma_multiplication: recalculate the grid if `stddev` larger than `sigma_mean + sigma_multiplication * sigma`, type:float, Default:5\n",
    "        '''\n",
    "        \n",
    "        # clean temp file\n",
    "        self.clean_temp()\n",
    "        \n",
    "        if available_GPU == None:\n",
    "            def is_gpu_available():\n",
    "                np.save(os.getcwd()+'/multi_temp/gpu_available', [i for i in range(len(list(cuda.gpus)))])\n",
    "                \n",
    "            # check gpu condition\n",
    "            p = multiprocessing.Process(target = is_gpu_available)\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "            p.join()\n",
    "            \n",
    "            available_GPU =  np.load(os.getcwd() + '/multi_temp/gpu_available.npy')\n",
    "        \n",
    "        if len(available_GPU) == 0:\n",
    "            raise AssertionError(\"Your computer does not support GPU calculation.\")\n",
    "            \n",
    "        # number of trials\n",
    "        self.num_trials = num_trials\n",
    "            \n",
    "        self.depth = depth\n",
    "\n",
    "        # recalculate the grid if `stddev` larger than `sigma_mean + sigma_multiplication * sigma`\n",
    "        self.sigma_multiplication = sigma_multiplication\n",
    "        \n",
    "        # set up initial conditions\n",
    "        self.available_GPU = available_GPU\n",
    "\n",
    "        # initialize the preparing integrated function depend on its domain dimension\n",
    "        self.initial(my_func, domain)\n",
    "        \n",
    "        # initial domain\n",
    "        self.initial_domain = domain\n",
    "       \n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.configure_chunks()\n",
    "        MCresult = self.importance_sampling_iteration(self.initial_domain, 0)\n",
    "        \n",
    "        return MCresult\n",
    "    \n",
    "    def importance_sampling_iteration(self, domain, depth):\n",
    "        depth += 1\n",
    "        MCresult_chunks, large_std_chunk_id, MCresult_std_chunks = self.MCevaluate(domain)\n",
    "        print('{} hypercube(s) need(s) to be recalculated, to save time, try drastically increasing sigma_multiplication.'.format(len(large_std_chunk_id)))\n",
    "        if depth < self.depth:\n",
    "            for chunk_id in large_std_chunk_id:\n",
    "                # domain of this chunk\n",
    "                domain_next_level = self.chunk_domian(chunk_id, domain)\n",
    "                \n",
    "                # iteration\n",
    "                MCresult_chunks[chunk_id],MCresult_std_chunks[chunk_id] = self.importance_sampling_iteration(domain_next_level, depth)\n",
    "                \n",
    "        # Stop digging if there are no more large stddev chunk\n",
    "        if len(large_std_chunk_id) == 0:\n",
    "            return np.sum(MCresult_chunks,0), np.sqrt(np.sum(MCresult_std_chunks**2))\n",
    "\n",
    "        return np.sum(MCresult_chunks,0), np.sqrt(np.sum(MCresult_std_chunks**2))\n",
    "    \n",
    "    def MCevaluate(self, domain):\n",
    "\n",
    "        '''\n",
    "        Monte Carlo integration.\n",
    "        Parameters:\n",
    "            domain: the integration domain, type:list or numpy_array.\n",
    "        '''\n",
    "        \n",
    "        p={}\n",
    "        for i_batch in range(self.n_batches):\n",
    "            def multi_processing():\n",
    "                os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_batch)\n",
    "                result = []\n",
    "                for trial in range(self.num_trials):\n",
    "                    result.append(self.MCkernel(domain, i_batch))\n",
    "                    \n",
    "                result = np.array(result)\n",
    "                std_result = np.std(result,0)\n",
    "                mean_result = np.mean(result,0)\n",
    "                np.save(os.getcwd()+'/multi_temp/result'+str(i_batch), np.array(mean_result))\n",
    "                np.save(os.getcwd()+'/multi_temp/result_std'+str(i_batch), np.array(std_result))\n",
    "                \n",
    "            # start multi-processing to allocate     \n",
    "            p[i_batch] = multiprocessing.Process(target = multi_processing)\n",
    "            p[i_batch].daemon = True\n",
    "            p[i_batch].start()\n",
    "            \n",
    "        for i_batch in range(self.n_batches):   \n",
    "            p[i_batch].join()\n",
    "                \n",
    "        MCresult = []\n",
    "        MCresult_std = []\n",
    "        for i_batch in range(self.n_batches): \n",
    "            MCresult.append(np.load(os.getcwd()+'/multi_temp/result'+str(i_batch)+'.npy'))\n",
    "            MCresult_std.append(np.load(os.getcwd()+'/multi_temp/result_std'+str(i_batch)+'.npy'))\n",
    "        \n",
    "        MCresult, MCresult_std = np.concatenate(MCresult), np.array(MCresult_std)\n",
    "    \n",
    "        # find out the index of chunks that have very large stddev\n",
    "        threshold = np.mean(MCresult_std) + self.sigma_multiplication * np.std(MCresult_std)\n",
    "        len_std = len(MCresult_std[0])\n",
    "        large_std_chunk_id = np.concatenate([np.nonzero(MCresult_std[i] >= threshold)[0] + i*len_std for i in range(len(MCresult_std))])\n",
    "        return MCresult, large_std_chunk_id, np.concatenate(MCresult_std)\n",
    "        \n",
    "        \n",
    "    def initial(self, my_func, domain):\n",
    "        '''\n",
    "        To obtain proper initial conditions:\n",
    "            self.dim: number of free variables, type:int,\n",
    "            self.chunk_size: number of samplings in each chunk, type:int\n",
    "            self.n_grid: total number of d-dimensional samplings, type:int\n",
    "            self.n_batches: seperate data into n_batches parts, type:int\n",
    "        Parameters:\n",
    "            my_func: user defined multidimensional function, type:function\n",
    "            domain: integration domain, type:list/numpy_array, eg [[0,1]] or [[0,1],[0,1]]\n",
    "        '''    \n",
    "\n",
    "        # detect if enter a function             \n",
    "        if my_func == None:\n",
    "            raise AssertionError(\"Invalid input function\")\n",
    "        # the preparing integrated function\n",
    "        self.my_func = my_func\n",
    "\n",
    "        # detect if domain is in right form\n",
    "        if domain == None:\n",
    "            raise AssertionError(\"Please enter a domain\")\n",
    "        for temp in domain:\n",
    "            if len(temp) != 2:\n",
    "                raise AssertionError(\"Domain is incorrect\")\n",
    "            if temp[1] < temp[0]:\n",
    "                raise AssertionError(\"Domain [a,b] should satisfy b>a\")\n",
    "                \n",
    "        # integrating dimension\n",
    "        self.dim = len(domain)\n",
    "        \n",
    "        # get `total sampling number` and `sampling number in one chunk` depend on dimension of integral       \n",
    "        if self.dim == 1:\n",
    "            self.chunk_size_x = 10000\n",
    "            self.n_one_gpu = 99999\n",
    "            \n",
    "        elif self.dim == 2:\n",
    "            self.chunk_size_x = 100\n",
    "            self.n_one_gpu = 999\n",
    "            \n",
    "        elif self.dim == 3:\n",
    "            self.chunk_size_x = 35\n",
    "            self.n_one_gpu = 99\n",
    "\n",
    "        elif self.dim == 4:\n",
    "            self.chunk_size_x = 10\n",
    "            self.n_one_gpu = 20\n",
    "            \n",
    "        elif self.dim == 5:\n",
    "            self.chunk_size_x = 7\n",
    "            self.n_one_gpu = 13\n",
    "            \n",
    "        elif self.dim == 6:\n",
    "            self.chunk_size_x = 6\n",
    "            self.n_one_gpu = 7\n",
    "            \n",
    "        elif self.dim == 7:\n",
    "            self.chunk_size_x = 4\n",
    "            self.n_one_gpu = 6\n",
    "            \n",
    "        elif self.dim == 8:\n",
    "            self.chunk_size_x = 3\n",
    "            self.n_one_gpu = 5\n",
    "            \n",
    "        elif self.dim == 9:\n",
    "            self.chunk_size_x = 3\n",
    "            self.n_one_gpu = 4\n",
    "            \n",
    "        elif self.dim == 10:\n",
    "            self.chunk_size_x = 3\n",
    "            self.n_one_gpu = 3\n",
    "            \n",
    "        elif self.dim == 11:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 3\n",
    "            \n",
    "        elif self.dim == 12:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 3\n",
    "           \n",
    "        elif self.dim == 13:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 2\n",
    "            \n",
    "        elif self.dim == 14:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 2\n",
    "            \n",
    "        elif self.dim == 15:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 2\n",
    "            \n",
    "        elif self.dim == 16:\n",
    "            self.chunk_size_x = 2\n",
    "            self.n_one_gpu = 2\n",
    "            \n",
    "        else:\n",
    "            self.chunk_size_x = 1\n",
    "            self.n_one_gpu = 2\n",
    "        \n",
    "        n_gpu = len(self.available_GPU)\n",
    "        self.chunk_size_multiplier = math.floor((n_gpu*(self.n_one_gpu**self.dim))**(1/self.dim))\n",
    "        \n",
    "    def configure_chunks(self):\n",
    "        '''receieve self.dim, self.n_grid and self.chunk_size'''\n",
    "        \n",
    "        '''\n",
    "            below, `int(np.round())` can make sure you got the exact number, \n",
    "            eg: in Python, you may get 7.99999 from 64^(1/2)\n",
    "        '''\n",
    "        \n",
    "        self.chunk_size = self.chunk_size_x**self.dim\n",
    "        self.n_grid = (self.chunk_size_x*self.chunk_size_multiplier)**self.dim\n",
    "        \n",
    "        # number of samplings in one chunk along one dimension\n",
    "        self.n_grid_x_one_chunk = int(np.round(self.chunk_size**(1/self.dim)))\n",
    "        \n",
    "        # number of chunks\n",
    "        self.n_chunk = int(np.round(self.n_grid/self.chunk_size))\n",
    "        \n",
    "        # number of samplings along one dimension\n",
    "        self.n_grid_x = int(np.round(self.n_grid**(1/self.dim)))\n",
    "        \n",
    "        # number of chunks along one dimension\n",
    "        self.n_chunk_x = int(np.round(self.n_chunk**(1/self.dim)))\n",
    "        \n",
    "        # number of batches\n",
    "        self.n_batches = min([len(self.available_GPU), self.n_chunk])\n",
    "        \n",
    "        # batch_size\n",
    "        self.batch_size = int(np.ceil(self.n_chunk/self.n_batches))\n",
    "\n",
    "    def clean_temp(self):\n",
    "        folder = os.getcwd()+'/multi_temp/'\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "        # clean temp file\n",
    "        for the_file in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, the_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "                \n",
    "    def chunk_domian(self, chunk_id, original_domain):\n",
    "\n",
    "        '''\n",
    "        Return:\n",
    "            domain of integration in this chunk.\n",
    "        Parameters:\n",
    "            chunk_id: current chunk id, type:int.\n",
    "            original_domain: the domain of the previous original integration.\n",
    "        '''\n",
    "        \n",
    "        chunk_id_d_dim = np.unravel_index(chunk_id, [self.n_chunk_x for _ in range(self.dim)])\n",
    "        domain_range = np.array([(original_domain[idim][1] - original_domain[idim][0]) / self.n_chunk_x for idim in range(self.dim)], dtype=np.float64)\n",
    "        domain_left = np.array([original_domain[idim][0] + chunk_id_d_dim[idim] * domain_range[idim] for idim in range(self.dim)], dtype=np.float64)\n",
    "        current_domain = [[domain_left[i], domain_left[i] + domain_range[i]] for i in range(self.dim)]\n",
    "        return current_domain\n",
    "    \n",
    "    def MCkernel(self, domain, i_batch):\n",
    "\n",
    "        '''\n",
    "        Function:\n",
    "            multiprocessing Monte Carlo integration on specific GPU\n",
    "        Parameters:\n",
    "            domain: domain of the integral, eg: [[a,b],[c,d],...].\n",
    "            i_batch: the index of current GPU, type:int.\n",
    "        '''\n",
    "        fun = self.my_func\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        MCresult = cuda.device_array(batch_size,dtype=np.float64)\n",
    "        n_chunk = self.n_chunk\n",
    "        chunk_size = self.chunk_size\n",
    "        num_loops = chunk_size * batch_size\n",
    "        n_chunk_x = self.n_chunk_x\n",
    "        domain_range = np.array([(domain[j_dim][1] - domain[j_dim][0]) / n_chunk_x for j_dim in range(dim)],\\\n",
    "                                dtype = np.float64)\n",
    "        domain = np.array(domain,dtype=np.float64)\n",
    "        \n",
    "        # change one dimensional index into \n",
    "        @cuda.jit(device=True)\n",
    "        def oneD_to_nD(num_of_points_in_each_dim,new_i,digit_store):\n",
    "            j_dim_index = 0\n",
    "            a1 = new_i//num_of_points_in_each_dim\n",
    "            digit_store[j_dim_index] = new_i%num_of_points_in_each_dim\n",
    "            \n",
    "            # convert to n-dim index\n",
    "            for j_dim in range(dim):\n",
    "                j_dim_index+=1\n",
    "                if a1 != 0.:\n",
    "                    digit_store[j_dim+1] = a1%num_of_points_in_each_dim\n",
    "                    a1 = a1//num_of_points_in_each_dim\n",
    "            \n",
    "        @cuda.jit\n",
    "        def integration_kernel(num_loops,\\\n",
    "                               MCresult,\\\n",
    "                               chunk_size,\\\n",
    "                               n_chunk_x,\\\n",
    "                               domain,\\\n",
    "                               domain_range,\\\n",
    "                               batch_size,\\\n",
    "                               i_batch,\\\n",
    "                               rng_states,\\\n",
    "                               n_chunk):\n",
    "            \n",
    "            thread_id = cuda.grid(1)\n",
    "            if thread_id < batch_size:\n",
    "                chunk_id = thread_id + i_batch * batch_size\n",
    "            \n",
    "                if chunk_id < n_chunk:\n",
    "            \n",
    "                    # digit_store: local digits index for each thread\n",
    "                    digit_store = cuda.local.array(shape=dim, dtype=nb.int64)\n",
    "                    for i_temp in range(dim):\n",
    "                        digit_store[i_temp] = 0\n",
    "                    \n",
    "                    # convert one_d index to dim_d index\n",
    "                    # result will be stored in digit_store\n",
    "                    oneD_to_nD(n_chunk_x,chunk_id,digit_store)\n",
    "            \n",
    "                    # specisify the local domain\n",
    "                    domain_left = cuda.local.array(dim, dtype=nb.float64)\n",
    "                    for j_dim in range(dim):\n",
    "                        domain_left[j_dim] = domain[j_dim][0] + digit_store[j_dim] * domain_range[j_dim]\n",
    "            \n",
    "                    for i_sample in range(chunk_size):\n",
    "                        # x_tuple: local axis values for each thread\n",
    "                        x_tuple = cuda.local.array(dim, dtype=nb.float64)\n",
    "                \n",
    "                        for j_dim in range(dim):\n",
    "                            x_tuple[j_dim] = xoroshiro128p_uniform_float64(rng_states, thread_id)\\\n",
    "                                                            *domain_range[j_dim] + domain_left[j_dim]\n",
    "                \n",
    "                        # feed in values to user defined function\n",
    "                        cuda.atomic.add(MCresult, thread_id, fun(x_tuple))\n",
    "               \n",
    "        # Configure the blocks\n",
    "        threadsperblock = 16\n",
    "        blockspergrid = (batch_size + (threadsperblock - 1)) // threadsperblock\n",
    "        rng_states = create_xoroshiro128p_states(threadsperblock * blockspergrid, \\\n",
    "                                                 seed=random.sample(range(0,1000),1)[0])\n",
    "                    \n",
    "        # Start the kernel \n",
    "        integration_kernel[blockspergrid, threadsperblock](num_loops,\\\n",
    "                                                           MCresult,\\\n",
    "                                                           chunk_size,\\\n",
    "                                                           n_chunk_x,\\\n",
    "                                                           domain,\\\n",
    "                                                           domain_range,\\\n",
    "                                                           batch_size,\\\n",
    "                                                           i_batch,\\\n",
    "                                                           rng_states,\\\n",
    "                                                           n_chunk)\n",
    "        \n",
    "        # volumn of the domain\n",
    "        volumn = np.prod(domain_range)/chunk_size\n",
    "        \n",
    "        MCresult = MCresult.copy_to_host()\n",
    "        MCresult = volumn*MCresult\n",
    "        \n",
    "        return MCresult\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
